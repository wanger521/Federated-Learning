 # 第一章 简介
 * 人工智能的应用涉及：语音处理、图像处理、自然语言处理、人机博弈、自动驾驶和健康医疗等。
 * 分布式机器学习与其他分布式系统不同，需要考虑它对数据的鲁棒性，对算法精度的特别要求，以及泛化性
 
 #第二章 机器学习基础
 ## 模型类别
 * 从学习目标来看，分为回归分类排序问题
 * 从训练数据特征来看，有监督、半监督、无监督、弱监督学习
 * 从模型复杂度来看，分为线性和非线性模型
 * 从模型功能，氛围生成模型和判别模型
 
 ## 损失函数
 * 损失函数与误差函数有关，但具有更好的数学性质，例如连续 可导 凸性， 容易优化
 * 常见的分类损失函数
 ** Hinge损失函数：l(w;x,y)=max{0,1-yg(x;w)}
 ** 指数损失函数：l(w;x,y) = exp(-yg(x;w))
 ** 交叉熵损失函数：sigmoid给出二分类的概率，softmax给出多分类的概率，再定义交叉熵损失函数，l(w;x,y) = $-\sum_{z\in{-1,1}}I_{y=z}logP(Y=z|x;w)$
 最小化交叉熵损失函数等价于最大化预测函数g所对应的条件似然函数。
 
 ## 常用的机器学习模型
 * 线性模型
 * 核方法与支持向量机 通过核方法吧原始输入控件变换为一个高维空间，从而找到一个超平面，使得原分类问题线性可分，且间隔最大。
 * 决策树与boosting 生成树状结构的决策模型，boosting是集合模型
 * 神经网络 激活函数（sigmoid tanh ReLU）全连接神经网络在训练中，通常选取交叉熵损失函数，使用梯度下降法来求解模型参数
 包括 全连接网络、卷积神经网络、循环神经网络等
 
 ##理论
 * 我们希望泛化误差尽可能地小，这个包括优化误差、估计误差和近似误差
 
 # 第三章 分布式机器学习框架
 ## 概念
 * 分布式机器学习对应的三大情形：计算并行（多线程多机并行计算），训练数据并行（目前为止最常见，划分数据），模型训练并行（模型分开，对通信要求高）
 * 包含四大模块：数据与模型划分，单机优化，通信，数据与模型聚合
 
 ## 模型与数据划分
 * 训练样本划分：有放回的随机采样，置乱切分（无放回的随机采样）
 * 维度划分：不同节点不同维度，配合特定优化方法使用（例如坐标下降法）
 * 模型划分：需要考虑模型的结构特点，例如对于神经网络，可以横向逐层划分，也可以纵向跨层划分，或者每个节点存储骨干网络，其余数据进行分布式训练
 
 ## 单机优化模块
 * 最小化经验损失函数
 
 ## 通信
 * 迭代式mapreduce，基于参数服务器的通信拓扑，基于数据流的通信拓扑
 * 同步通信，异步通信，混合同步，半同步
 
 ## 数据与模型聚合
 * 模型平均，解一致性优化我呢提（如ADMM），还可以模型集成。凸问题可以保证精度，非凸很难。对于、nonidd问题，可以模型集成，就是会增大模型规模
 
 ## 总结
 * 理论性一般讨论收敛性，加速比，泛化性
 * 本书结构
 
 # 第四章 单机优化之确定性算法
 ## 概述
 * 收敛速率 最优模型与迭代出的模型之间的距离 或者对应的正则化经验风险的差值来衡量：$E||w_T-w^*||^2 <= \epsilon(T)$
 * 如果log(\epsilon(T)) 与 -T同阶，则是线性收敛速率；比衰减速度慢，则次线性；快，则超线性。若loglog(\epsilon(T))与-T同阶，则具有二阶收敛速率。
 * 基本假设：凸函数，强凸函数，Lipschitz连续（通常对于不可导函数假设这个来描述光滑性，which刻画函数的缓急程度），光滑函数（梯度）
 * 优化算法的分类：梯度下降法，共轭梯度法，坐标下降法，牛顿法，拟牛顿法，Frank-Wolfe方法，Nesterov加速法，内点法，对偶方法等。
 * 依据是否对于数据或者变量的维度进行了随机采样，把优化算法氛围确定性算法和随机算法
 * 依据利用是一阶导数还是二阶导数的信息，分为一阶算法和二阶算法
 * 依据优化算法在原问题空间还是在对偶空间进行优化，分为原始方法和对偶方法
## 一阶确定性算法
### 梯度下降法
* 基本思想：最小化目标函数在当前状态的一阶泰勒展开
* min_w f(w) yue= minf(w_t) +\nabla f(w_t)^T(w-w_t),使得\nabla f(w_t)^T w最小的方向是与梯度\nabla f(w_t)方向相反，故更新规则如下：
* w_{t+1}=w_t-\eta \nabla f(w_t)
* 凸加光滑：次线性收敛速度，强凸加光滑：线性收敛速度，Q = L/mu,条件数越小，收敛越快
* 局限性：只适用于无约束的优化问题，只适用于梯度存在

### 投影次梯度下降法
* 先从次梯度中选一个，然后做梯度下降，然后投影回约束域。
* 适用于算不出梯度的，所以可以在Lipschitz连续的假设下用此算法
* 在光滑的假设下，额外加凸则收敛速度是次线性，加强凸是线性
* 在Lipschitz的假设下，额外加凸和非凸再使用递减步长，收敛速度都是次线性，收敛速度显著下降

### 近端梯度下降法
* 常用于解决： min f(w) = l(w) + R(w), l可微，R不可微但凸， w_{t+1}=prox_{\eta_t R}(w_t - \eta_t \nabla l(w_t))
* 在光滑强凸假设下，具有线性收敛速度

### Frank-Wolfe
* 是投影次梯度下降法的替代，解决投影太复杂的情况
* Frank-Wolfe算法在最小化目标函数的泰勒展开时就将约束条件考虑进去，直接得到满足约束的近似最小点
* 先计算梯度，再计算 v_t = argmin_{v}\nabla f(w_t)^T v,再为了稳定性做线性加权 w_{t+1} = (1-gamma)w_t+gamma v_t
* 光滑加凸，达到次线性收敛

### Nesterov加速方法
* 在lipschitz连续的情况下，梯度下降法已经达到了一节算法收敛速率的下届，无改善空间
* 在光滑情况下，还未达到，因为采用nesterov来加速一阶算法收敛，收敛到证得的下届
* 按照一定步长计算辅助变量 v_{t+1},然后根据凸和强凸，线性加权当前辅助变量和上一个辅助变量。

### 坐标下降法
* 每次最小化一块维度
* 强凸加光滑，具有线性收敛 

## 二阶确定性算法
### 牛顿法
* 基本思想是将目标函数在当前状态进行二阶泰勒展开，然后最小化这个近似目标函数，
如果在当前状态w_t处的海森矩阵 \nabla^2f(w_t)是正定的，上述优化问题的最优值在w_t-[\nabla^2f(w_t)]^-1 \nabla f(w_t)处取到
* 牛顿法把这个作为下一时刻的状态，即 w_t+1 = w_t - [\nabla^2f(w_t)]^-1 \nabla f(w_t)
* 相比一阶，二阶的牛顿法提供了更为精细的步长调节，海森矩阵的逆矩阵
* 具有二阶收敛速度 在导数光滑时
* 海森逆计算量大 存储量大 且不一定是正定的
### 拟牛顿法
* 构造一个于海森矩阵相差不太远的正定矩阵作为替代，可以迭代更新海森矩阵
* BFGS算法， 当初始点离最优点足够近时，拟牛顿法和牛顿法同样具有二阶收敛速度。
### 对偶方法
* 先把原始问题转为对偶问题
* 弱对偶： 对偶问题的解小于原问题 强队偶则是等于
* 对偶问题是个maxinf问题
* inf函数是保凹的
* KKT条件是强对偶的必要条件
* 如对偶坐标上升法， 对于具有线性约束的凸优化问题，该算法至少有线性收敛速度

 ## 总结
 * 更好的凸性和更好的光滑性质会加速算法收敛（条件数越小，强凸系数越大，光滑系数越小）
 * 随机算法建立在确定算法之上
 
