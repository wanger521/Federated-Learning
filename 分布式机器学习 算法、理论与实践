 # 第一章 简介
 * 人工智能的应用涉及：语音处理、图像处理、自然语言处理、人机博弈、自动驾驶和健康医疗等。
 * 分布式机器学习与其他分布式系统不同，需要考虑它对数据的鲁棒性，对算法精度的特别要求，以及泛化性
 
 #第二章 机器学习基础
 ## 模型类别
 * 从学习目标来看，分为回归分类排序问题
 * 从训练数据特征来看，有监督、半监督、无监督、弱监督学习
 * 从模型复杂度来看，分为线性和非线性模型
 * 从模型功能，氛围生成模型和判别模型
 
 ## 损失函数
 * 损失函数与误差函数有关，但具有更好的数学性质，例如连续 可导 凸性， 容易优化
 * 常见的分类损失函数
 ** Hinge损失函数：l(w;x,y)=max{0,1-yg(x;w)}
 ** 指数损失函数：l(w;x,y) = exp(-yg(x;w))
 ** 交叉熵损失函数：sigmoid给出二分类的概率，softmax给出多分类的概率，再定义交叉熵损失函数，l(w;x,y) = $-\sum_{z\in{-1,1}}I_{y=z}logP(Y=z|x;w)$
 最小化交叉熵损失函数等价于最大化预测函数g所对应的条件似然函数。
 
 ## 常用的机器学习模型
 * 线性模型
 * 核方法与支持向量机 通过核方法吧原始输入控件变换为一个高维空间，从而找到一个超平面，使得原分类问题线性可分，且间隔最大。
 * 决策树与boosting 生成树状结构的决策模型，boosting是集合模型
 * 神经网络 激活函数（sigmoid tanh ReLU）全连接神经网络在训练中，通常选取交叉熵损失函数，使用梯度下降法来求解模型参数
 包括 全连接网络、卷积神经网络、循环神经网络等
 
 ##理论
 * 我们希望泛化误差尽可能地小，这个包括优化误差、估计误差和近似误差
 
 # 第三章 分布式机器学习框架
 ## 概念
 * 分布式机器学习对应的三大情形：计算并行（多线程多机并行计算），训练数据并行（目前为止最常见，划分数据），模型训练并行（模型分开，对通信要求高）
 * 包含四大模块：数据与模型划分，单机优化，通信，数据与模型聚合
 
 ## 模型与数据划分
 * 训练样本划分：有放回的随机采样，置乱切分（无放回的随机采样）
 * 维度划分：不同节点不同维度，配合特定优化方法使用（例如坐标下降法）
 * 模型划分：需要考虑模型的结构特点，例如对于神经网络，可以横向逐层划分，也可以纵向跨层划分，或者每个节点存储骨干网络，其余数据进行分布式训练
 
 ## 单机优化模块
 * 最小化经验损失函数
 
 ## 通信
 * 迭代式mapreduce，基于参数服务器的通信拓扑，基于数据流的通信拓扑
 * 同步通信，异步通信，混合同步，半同步
 
 ## 数据与模型聚合
 * 模型平均，解一致性优化我呢提（如ADMM），还可以模型集成。凸问题可以保证精度，非凸很难。对于、nonidd问题，可以模型集成，就是会增大模型规模
 
 ## 总结
 * 理论性一般讨论收敛性，加速比，泛化性
 * 本书结构
 
 # 第四章 单机优化之确定性算法
 ## 概述
 * 收敛速率 最优模型与迭代出的模型之间的距离 或者对应的正则化经验风险的差值来衡量：$E||w_T-w^*||^2 <= \epsilon(T)$
 * 如果log(\epsilon(T)) 与 -T同阶，则是线性收敛速率；比衰减速度慢，则次线性；快，则超线性。若loglog(\epsilon(T))与-T同阶，则具有二阶收敛速率。
 * 基本假设：凸函数，强凸函数，Lipschitz连续（通常对于不可导函数假设这个来描述光滑性，which刻画函数的缓急程度），光滑函数（梯度）
 * 优化算法的分类：梯度下降法，共轭梯度法，坐标下降法，牛顿法，拟牛顿法，Frank-Wolfe方法，Nesterov加速法，内点法，对偶方法等。
 * 依据是否对于数据或者变量的维度进行了随机采样，把优化算法氛围确定性算法和随机算法
 * 依据利用是一阶导数还是二阶导数的信息，分为一阶算法和二阶算法
 * 依据优化算法在原问题空间还是在对偶空间进行优化，分为原始方法和对偶方法
## 一阶确定性算法
### 梯度下降法
* 基本思想：最小化目标函数在当前状态的一阶泰勒展开
* min_w f(w) yue= minf(w_t) +\nabla f(w_t)^T(w-w_t),使得\nabla f(w_t)^T w最小的方向是与梯度\nabla f(w_t)方向相反，故更新规则如下：
* w_{t+1}=w_t-\eta \nabla f(w_t)
* 凸加光滑：次线性收敛速度，强凸加光滑：线性收敛速度，Q = L/mu,条件数越小，收敛越快
* 局限性：只适用于无约束的优化问题，只适用于梯度存在

### 投影次梯度下降法
* 先从次梯度中选一个，然后做梯度下降，然后投影回约束域。
* 适用于算不出梯度的，所以可以在Lipschitz连续的假设下用此算法
* 在光滑的假设下，额外加凸则收敛速度是次线性，加强凸是线性
* 在Lipschitz的假设下，额外加凸和非凸再使用递减步长，收敛速度都是次线性，收敛速度显著下降

### 近端梯度下降法

